apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-18T22:07:31Z"
    generateName: r4-infrastructure-prometheus-alertmanager-fb95778b-
    labels:
      app: prometheus
      chart: prometheus-11.3.0
      component: alertmanager
      heritage: Helm
      pod-template-hash: fb95778b
      release: r4-infrastructure-prometheus
    name: r4-infrastructure-prometheus-alertmanager-fb95778b-762bm
    namespace: ricplt
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: r4-infrastructure-prometheus-alertmanager-fb95778b
      uid: 67db4ea3-ad70-4e40-944e-25ecbef4753c
    resourceVersion: "29220"
    uid: c4b3ee78-5a07-4a20-a238-d0a25a4d8636
  spec:
    containers:
    - args:
      - --config.file=/etc/config/alertmanager.yml
      - --storage.path=/data
      - --cluster.advertise-address=$(POD_IP):6783
      - --web.external-url=http://localhost:9093
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: prom/alertmanager:v0.20.0
      imagePullPolicy: IfNotPresent
      name: prometheus-alertmanager
      ports:
      - containerPort: 9093
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9093
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5rg8l
        readOnly: true
    - args:
      - --volume-dir=/etc/config
      - --webhook-url=http://127.0.0.1:9093/-/reload
      image: jimmidyson/configmap-reload:v0.3.0
      imagePullPolicy: IfNotPresent
      name: prometheus-alertmanager-configmap-reload
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5rg8l
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: mbwcl711-3060-system-product-name
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: r4-infrastructure-prometheus-alertmanager
    serviceAccountName: r4-infrastructure-prometheus-alertmanager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: r4-infrastructure-prometheus-alertmanager
      name: config-volume
    - emptyDir: {}
      name: storage-volume
    - name: kube-api-access-5rg8l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T22:07:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T22:08:02Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T22:08:02Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T22:07:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://fb0bd803a7eed8fc3add8d7fdd54c3c9da57905a4505fdf440c096e61588c780
      image: docker.io/prom/alertmanager:v0.20.0
      imageID: docker.io/prom/alertmanager@sha256:7e4e9f7a0954b45736d149c40e9620a6664036bb05f0dce447bef5042b139f5d
      lastState: {}
      name: prometheus-alertmanager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T22:07:32Z"
    - containerID: containerd://e898e96df9dd953d9e4df5688cd570e68ab17d83b88776c3579eda7ea8a1b0c0
      image: docker.io/jimmidyson/configmap-reload:v0.3.0
      imageID: docker.io/jimmidyson/configmap-reload@sha256:d107c7a235c266273b1c3502a391fec374430e5625539403d0de797fa9c556a2
      lastState: {}
      name: prometheus-alertmanager-configmap-reload
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T22:07:32Z"
    hostIP: 192.168.0.190
    phase: Running
    podIP: 10.42.0.75
    podIPs:
    - ip: 10.42.0.75
    qosClass: Burstable
    startTime: "2025-11-18T22:07:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-18T22:07:31Z"
    generateName: r4-infrastructure-prometheus-server-6c4cbf94d4-
    labels:
      app: prometheus
      chart: prometheus-11.3.0
      component: server
      heritage: Helm
      pod-template-hash: 6c4cbf94d4
      release: r4-infrastructure-prometheus
    name: r4-infrastructure-prometheus-server-6c4cbf94d4-lm9x9
    namespace: ricplt
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: r4-infrastructure-prometheus-server-6c4cbf94d4
      uid: ba63ffe6-ffa9-4257-8706-6806abe5caaa
    resourceVersion: "29224"
    uid: 60258352-1a86-46d4-893c-f8836d88c69c
  spec:
    containers:
    - args:
      - --storage.tsdb.retention.time=15d
      - --config.file=/etc/config/prometheus.yml
      - --storage.tsdb.path=/data
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --web.console.templates=/etc/prometheus/consoles
      - --web.enable-lifecycle
      image: prom/prometheus:v2.18.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: prometheus-server
      ports:
      - containerPort: 9090
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 1Gi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-clhgv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: mbwcl711-3060-system-product-name
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: r4-infrastructure-prometheus-server
    serviceAccountName: r4-infrastructure-prometheus-server
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: r4-infrastructure-prometheus-server
      name: config-volume
    - emptyDir: {}
      name: storage-volume
    - name: kube-api-access-clhgv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T22:07:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T22:08:02Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T22:08:02Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T22:07:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0641fe78d08a39dd620ec3a86a679042ef7884692d2e8aa5018b788dd75dae33
      image: docker.io/prom/prometheus:v2.18.1
      imageID: docker.io/prom/prometheus@sha256:5880ec936055fad18ccee798d2a63f64ed85bd28e8e0af17c6923a090b686c3d
      lastState: {}
      name: prometheus-server
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T22:07:32Z"
    hostIP: 192.168.0.190
    phase: Running
    podIP: 10.42.0.252
    podIPs:
    - ip: 10.42.0.252
    qosClass: Burstable
    startTime: "2025-11-18T22:07:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 4b77ec1e5cb951a8be2ba3bbd035d2fd52038e854772874290cc5e974424081f
      checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
      checksum/secret: 9cf1ef28d12bd4584a51aa71531a1ce2b77313c2ad880144bdf0c8e35e8bd2e8
      kubectl.kubernetes.io/default-container: grafana
    creationTimestamp: "2025-11-18T22:07:53Z"
    generateName: oran-grafana-f6bb8ff8f-
    labels:
      app.kubernetes.io/instance: oran-grafana
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.2.1
      helm.sh/chart: grafana-10.1.4
      pod-template-hash: f6bb8ff8f
    name: oran-grafana-f6bb8ff8f-249rb
    namespace: ricplt
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: oran-grafana-f6bb8ff8f
      uid: 063a0987-7ce7-4625-aff8-7bc072bc2cd7
    resourceVersion: "29231"
    uid: 8c155efc-d59e-4784-9c12-deb0118e241c
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: oran-grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: oran-grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      - name: GOMEMLIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.memory
      - name: GF_INSTALL_PLUGINS
      image: docker.io/grafana/grafana:12.2.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      - containerPort: 9094
        name: gossip-tcp
        protocol: TCP
      - containerPort: 9094
        name: gossip-udp
        protocol: UDP
      - containerPort: 6060
        name: profiling
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /etc/grafana/provisioning/datasources/datasources.yaml
        name: config
        subPath: datasources.yaml
      - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
        name: config
        subPath: dashboardproviders.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nm59m
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: mbwcl711-3060-system-product-name
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsNonRoot: true
      runAsUser: 472
    serviceAccount: oran-grafana
    serviceAccountName: oran-grafana
    shareProcessNamespace: false
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: oran-grafana
      name: config
    - emptyDir: {}
      name: storage
    - name: kube-api-access-nm59m
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T22:07:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T22:08:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T22:08:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T22:07:53Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://51c079c4a68aaa63880052dc26989415e1e35854ba00b2ba0c7d6ee34709b8aa
      image: docker.io/grafana/grafana:12.2.1
      imageID: docker.io/grafana/grafana@sha256:35c41e0fd0295f5d0ee5db7e780cf33506abfaf47686196f825364889dee878b
      lastState: {}
      name: grafana
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T22:07:54Z"
    hostIP: 192.168.0.190
    phase: Running
    podIP: 10.42.0.56
    podIPs:
    - ip: 10.42.0.56
    qosClass: Burstable
    startTime: "2025-11-18T22:07:53Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: r4-infrastructure-prometheus
      meta.helm.sh/release-namespace: ricplt
    creationTimestamp: "2025-11-18T22:07:31Z"
    labels:
      app: prometheus
      app.kubernetes.io/managed-by: Helm
      chart: prometheus-11.3.0
      component: server
      heritage: Helm
      release: r4-infrastructure-prometheus
    name: r4-infrastructure-prometheus-server
    namespace: ricplt
    resourceVersion: "29118"
    uid: 593550cd-7c5e-44bf-8a6b-3728873a12ef
  spec:
    clusterIP: 10.43.194.85
    clusterIPs:
    - 10.43.194.85
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9090
    selector:
      app: prometheus
      component: server
      release: r4-infrastructure-prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: r4-infrastructure-prometheus
      meta.helm.sh/release-namespace: ricplt
    creationTimestamp: "2025-11-18T22:07:31Z"
    labels:
      app: prometheus
      app.kubernetes.io/managed-by: Helm
      chart: prometheus-11.3.0
      component: alertmanager
      heritage: Helm
      release: r4-infrastructure-prometheus
    name: r4-infrastructure-prometheus-alertmanager
    namespace: ricplt
    resourceVersion: "29120"
    uid: 2283119b-1e75-438e-b709-000959258962
  spec:
    clusterIP: 10.43.77.46
    clusterIPs:
    - 10.43.77.46
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9093
    selector:
      app: prometheus
      component: alertmanager
      release: r4-infrastructure-prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: oran-grafana
      meta.helm.sh/release-namespace: ricplt
    creationTimestamp: "2025-11-18T22:07:53Z"
    labels:
      app.kubernetes.io/instance: oran-grafana
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.2.1
      helm.sh/chart: grafana-10.1.4
    name: oran-grafana
    namespace: ricplt
    resourceVersion: "29189"
    uid: f7837a19-2c9a-458f-92d9-9a7ff0b17121
  spec:
    clusterIP: 10.43.105.76
    clusterIPs:
    - 10.43.105.76
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: grafana
    selector:
      app.kubernetes.io/instance: oran-grafana
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: r4-infrastructure-prometheus
      meta.helm.sh/release-namespace: ricplt
    creationTimestamp: "2025-11-18T22:07:31Z"
    generation: 1
    labels:
      app: prometheus
      app.kubernetes.io/managed-by: Helm
      chart: prometheus-11.3.0
      component: alertmanager
      heritage: Helm
      release: r4-infrastructure-prometheus
    name: r4-infrastructure-prometheus-alertmanager
    namespace: ricplt
    resourceVersion: "29225"
    uid: 5c08d6a3-af96-46dc-bee9-3f73bd1a3a5b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: prometheus
        component: alertmanager
        release: r4-infrastructure-prometheus
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: prometheus
          chart: prometheus-11.3.0
          component: alertmanager
          heritage: Helm
          release: r4-infrastructure-prometheus
      spec:
        containers:
        - args:
          - --config.file=/etc/config/alertmanager.yml
          - --storage.path=/data
          - --cluster.advertise-address=$(POD_IP):6783
          - --web.external-url=http://localhost:9093
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: prom/alertmanager:v0.20.0
          imagePullPolicy: IfNotPresent
          name: prometheus-alertmanager
          ports:
          - containerPort: 9093
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9093
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        - args:
          - --volume-dir=/etc/config
          - --webhook-url=http://127.0.0.1:9093/-/reload
          image: jimmidyson/configmap-reload:v0.3.0
          imagePullPolicy: IfNotPresent
          name: prometheus-alertmanager-configmap-reload
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: r4-infrastructure-prometheus-alertmanager
        serviceAccountName: r4-infrastructure-prometheus-alertmanager
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: r4-infrastructure-prometheus-alertmanager
          name: config-volume
        - emptyDir: {}
          name: storage-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-18T22:08:02Z"
      lastUpdateTime: "2025-11-18T22:08:02Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-18T22:07:31Z"
      lastUpdateTime: "2025-11-18T22:08:02Z"
      message: ReplicaSet "r4-infrastructure-prometheus-alertmanager-fb95778b" has
        successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: r4-infrastructure-prometheus
      meta.helm.sh/release-namespace: ricplt
    creationTimestamp: "2025-11-18T22:07:31Z"
    generation: 1
    labels:
      app: prometheus
      app.kubernetes.io/managed-by: Helm
      chart: prometheus-11.3.0
      component: server
      heritage: Helm
      release: r4-infrastructure-prometheus
    name: r4-infrastructure-prometheus-server
    namespace: ricplt
    resourceVersion: "29229"
    uid: cf14dd48-9025-4a40-a2d2-a283c706048b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: prometheus
        component: server
        release: r4-infrastructure-prometheus
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: prometheus
          chart: prometheus-11.3.0
          component: server
          heritage: Helm
          release: r4-infrastructure-prometheus
      spec:
        containers:
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: prom/prometheus:v2.18.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: r4-infrastructure-prometheus-server
        serviceAccountName: r4-infrastructure-prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: r4-infrastructure-prometheus-server
          name: config-volume
        - emptyDir: {}
          name: storage-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-18T22:08:02Z"
      lastUpdateTime: "2025-11-18T22:08:02Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-18T22:07:31Z"
      lastUpdateTime: "2025-11-18T22:08:02Z"
      message: ReplicaSet "r4-infrastructure-prometheus-server-6c4cbf94d4" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: oran-grafana
      meta.helm.sh/release-namespace: ricplt
    creationTimestamp: "2025-11-18T22:07:53Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: oran-grafana
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.2.1
      helm.sh/chart: grafana-10.1.4
    name: oran-grafana
    namespace: ricplt
    resourceVersion: "29235"
    uid: 76919dd4-461d-49c6-ae93-fcedee142b40
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: oran-grafana
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 4b77ec1e5cb951a8be2ba3bbd035d2fd52038e854772874290cc5e974424081f
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 9cf1ef28d12bd4584a51aa71531a1ce2b77313c2ad880144bdf0c8e35e8bd2e8
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: oran-grafana
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.2.1
          helm.sh/chart: grafana-10.1.4
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: oran-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: oran-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.memory
          - name: GF_INSTALL_PLUGINS
          image: docker.io/grafana/grafana:12.2.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /etc/grafana/provisioning/datasources/datasources.yaml
            name: config
            subPath: datasources.yaml
          - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
            name: config
            subPath: dashboardproviders.yaml
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: oran-grafana
        serviceAccountName: oran-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: oran-grafana
          name: config
        - emptyDir: {}
          name: storage
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-18T22:08:04Z"
      lastUpdateTime: "2025-11-18T22:08:04Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-18T22:07:53Z"
      lastUpdateTime: "2025-11-18T22:08:04Z"
      message: ReplicaSet "oran-grafana-f6bb8ff8f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: r4-infrastructure-prometheus
      meta.helm.sh/release-namespace: ricplt
    creationTimestamp: "2025-11-18T22:07:31Z"
    generation: 1
    labels:
      app: prometheus
      chart: prometheus-11.3.0
      component: alertmanager
      heritage: Helm
      pod-template-hash: fb95778b
      release: r4-infrastructure-prometheus
    name: r4-infrastructure-prometheus-alertmanager-fb95778b
    namespace: ricplt
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: r4-infrastructure-prometheus-alertmanager
      uid: 5c08d6a3-af96-46dc-bee9-3f73bd1a3a5b
    resourceVersion: "29223"
    uid: 67db4ea3-ad70-4e40-944e-25ecbef4753c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: prometheus
        component: alertmanager
        pod-template-hash: fb95778b
        release: r4-infrastructure-prometheus
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: prometheus
          chart: prometheus-11.3.0
          component: alertmanager
          heritage: Helm
          pod-template-hash: fb95778b
          release: r4-infrastructure-prometheus
      spec:
        containers:
        - args:
          - --config.file=/etc/config/alertmanager.yml
          - --storage.path=/data
          - --cluster.advertise-address=$(POD_IP):6783
          - --web.external-url=http://localhost:9093
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: prom/alertmanager:v0.20.0
          imagePullPolicy: IfNotPresent
          name: prometheus-alertmanager
          ports:
          - containerPort: 9093
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9093
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        - args:
          - --volume-dir=/etc/config
          - --webhook-url=http://127.0.0.1:9093/-/reload
          image: jimmidyson/configmap-reload:v0.3.0
          imagePullPolicy: IfNotPresent
          name: prometheus-alertmanager-configmap-reload
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: r4-infrastructure-prometheus-alertmanager
        serviceAccountName: r4-infrastructure-prometheus-alertmanager
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: r4-infrastructure-prometheus-alertmanager
          name: config-volume
        - emptyDir: {}
          name: storage-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: r4-infrastructure-prometheus
      meta.helm.sh/release-namespace: ricplt
    creationTimestamp: "2025-11-18T22:07:31Z"
    generation: 1
    labels:
      app: prometheus
      chart: prometheus-11.3.0
      component: server
      heritage: Helm
      pod-template-hash: 6c4cbf94d4
      release: r4-infrastructure-prometheus
    name: r4-infrastructure-prometheus-server-6c4cbf94d4
    namespace: ricplt
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: r4-infrastructure-prometheus-server
      uid: cf14dd48-9025-4a40-a2d2-a283c706048b
    resourceVersion: "29227"
    uid: ba63ffe6-ffa9-4257-8706-6806abe5caaa
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: prometheus
        component: server
        pod-template-hash: 6c4cbf94d4
        release: r4-infrastructure-prometheus
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: prometheus
          chart: prometheus-11.3.0
          component: server
          heritage: Helm
          pod-template-hash: 6c4cbf94d4
          release: r4-infrastructure-prometheus
      spec:
        containers:
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: prom/prometheus:v2.18.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: r4-infrastructure-prometheus-server
        serviceAccountName: r4-infrastructure-prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: r4-infrastructure-prometheus-server
          name: config-volume
        - emptyDir: {}
          name: storage-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: oran-grafana
      meta.helm.sh/release-namespace: ricplt
    creationTimestamp: "2025-11-18T22:07:53Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: oran-grafana
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.2.1
      helm.sh/chart: grafana-10.1.4
      pod-template-hash: f6bb8ff8f
    name: oran-grafana-f6bb8ff8f
    namespace: ricplt
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: oran-grafana
      uid: 76919dd4-461d-49c6-ae93-fcedee142b40
    resourceVersion: "29233"
    uid: 063a0987-7ce7-4625-aff8-7bc072bc2cd7
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: oran-grafana
        app.kubernetes.io/name: grafana
        pod-template-hash: f6bb8ff8f
    template:
      metadata:
        annotations:
          checksum/config: 4b77ec1e5cb951a8be2ba3bbd035d2fd52038e854772874290cc5e974424081f
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 9cf1ef28d12bd4584a51aa71531a1ce2b77313c2ad880144bdf0c8e35e8bd2e8
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: oran-grafana
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.2.1
          helm.sh/chart: grafana-10.1.4
          pod-template-hash: f6bb8ff8f
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: oran-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: oran-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.memory
          - name: GF_INSTALL_PLUGINS
          image: docker.io/grafana/grafana:12.2.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /etc/grafana/provisioning/datasources/datasources.yaml
            name: config
            subPath: datasources.yaml
          - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
            name: config
            subPath: dashboardproviders.yaml
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: oran-grafana
        serviceAccountName: oran-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: oran-grafana
          name: config
        - emptyDir: {}
          name: storage
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
kind: List
metadata:
  resourceVersion: ""
